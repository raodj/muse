\chapter{Introduction} 

Discrete event simulation (DES) is a computational methodology for
modeling and analysis of a wide spectrum of systems.  In DES, the
system being modeled is logically subdivided into small, independent,
but interacting entities with their own independent states.  The
model and implementation of an entity is called a Logical Process
(LP), which manage the state assocated with them.  Accordingly, a DES
is essentially designed as a set of logical processes (LPs) that
interact with each other.  LPs interact by exchanging and processing
discrete-timestamped events or messages~\cite{jafer-13}.  Processing
an event essentially introduces a change in an LP's state and causes
the LP to generate additional events to itself or other LPs in the
model.

A key aspect of DES is that state changes occur at discrete
times~\cite{fishman-13}.  At each point in time in a simulation, a
virtual time-stamp is assigned to an event and the event precipitates
a transition from one state to another state. This change in system
state is used to represent the dynamic nature and behavior of a
real-world system~\cite{fujimoto-90}.

DES has been used in a variety of fields in academia, industry, and the
public sector as a tool to help inform knowledge and to improve
decision-making processes~\cite{fishman-13}. DES provides an effective
means for analyzing real or artificial systems without the constraint
of limited resources such as time, financial costs, or safety. For
example, the simulation of a battlefield environment can deliver
insightful information to military planners on enemy troop movements,
tactics, and capabilities during strategic planning
efforts~\cite{hill-01}. A discrete event simulation of the battlefield
allows military leaders to examine the impacts of decisions without
the real-world risks associated with committing forces to dangerous
environments.

\section{Parallel Simulation}

Parallelism in computing frameworks that support DES increase
performance throughput that is needed to construct and execute large
scale and complex simulation models. With the growth and prevalence of
semiconductor technology, cheaper and powerful multi-processors can be
instrumented to achieve greater computing power for parallel discrete
event simulations (PDES)~\cite{pacheco2011introduction, bryant2003computer}.  In parallel simulations, LPs are subdivided
or partitioned to operate on different compute units.  However, event
processing on the different compute units must be synchronized to
ensure causally consistent event processing.  Consequently, the
speedup achieved using multi-core and multi-processor systems requires
efficient strategies to minimize synchronization costs.

Currently, two broad types of synchronization methods are used in
PDES, namely: conservative and optimistic approaches~\cite{jafer-13}.
Conservative methods tightly coordinate event processing so that
causal violations do not occur.  Optimistic methods, such as Time
Warp~\cite{jafer-13}, loosely synchronize LPs -- they permit temporary
causal violations to occur but detect and recover from causal
violations.  Recently, optimistic synchronization methods have outperformed conservative methods for certain classes of
systems~\cite{jafer-13}.


\section{Managing Pending Events}

Sequential and parallel DES are designed as a set of logical processes
(LPs) that interact with each other by exchanging and processing
timestamped events or messages~\cite{jafer-13}. Events that are yet to
be processed are called "pending events". Pending events must be
processed by LPs in priority order to maintain causality, with event
priorities being determined by their timestamps. Consequently, data
structures for managing and prioritizing pending events play a
critical role in ensuring efficient sequential and parallel
simulations~\cite{jones-86,ronngren-97,brown-88,franceschini-15}. The
effectiveness of data structures for event management is a conspicuous
issue in larger simulations, where thousands or millions of events can
be pending~\cite{carothers-2010,yeom-14}. Large pending event sets can
arise when a model has many LPs or when each LP generates / processes
many events. Overheads in managing pending events is magnified in fine
grained simulations where the time taken to process an event is very
short. Furthermore, the synchronization strategy used in Time Warp (an
optimistic synchronization strategy) can further impact the
effectiveness of the data structure due to additional processing
required during rollback-based recovery operations.

\section{Thesis Statement}

This research proposes and explorers multi-tier data structures for
the improved management of the pending event set in sequential and
optimistic parallel simulations.  The objective of the research is to
develop and assess effectiveness of novel data structures for managing
pending events.  Specifically, this thesis proposes multi-tiered data
structures called 2-tier Ladder Queue (2tLadderQ) and 3-tier Heap
(3tHeap) for managing pending events.  We conduct experimental assessment of the
proposed data structures by comparing their
effectiveness using benchmark simulations and a fine-tuned version of
the Ladder Queue~\cite{tang-05}. We use the Ladder Queue, with amortized
O(1) time complexity for comparison because it has
shown to to be very efficient for sequential DES. \newline

\textbf{Thesis:} The multi-tiered, 
\textbf{2tLadderQ} and \textbf{3tHeap} pending event structures
outperform other priority queue based implementation of the pending event set, specifically, \textbf{Ladder Queue}, \textbf{Binary Heap}, \textbf{Binomial Heap}, \textbf{Fibonacci Heap} and \textbf{2-Tier Heap}. The contributions of the thesis are the development of two novel data structures \textbf{2tLadderQ} and \textbf{3tHeap} and the identification of key influential model characteristics for determining the choice of scheduler queue.

\chapter{Background and Related Work}

Many investigations have explored the effectiveness of a wide variety
of data structures for managing the pending event set in sequential
and parallel discrete event simulation (PDES).  The prior
investigations in PDES area fall under two broad categorizes, namely:
shared memory versus distributed memory data structures. Shared
memory data structures focus on managing pending events in PDES that
use multiple threads for parallelism.  Such simulations are typically
performed on large shared memory machines with many-core CPUs or
dedicated GPGPUs or coprocessors. These data structures focus on
enabling concurrent access to add or remove pending events from
multiple threads while avoiding race conditions.  Race conditions are
voided using conventional lock-based approaches such as semaphores or
mutexes. Recently, lock-free data structures based on special
check-and-set (CAS) instructions have also been proposed to enable
efficient, thread-safe, and concurrent access.

Distributed memory data structures focus on enabling efficient
single-threaded operation. However, these data structures need to
enable managing events received over communication channels from other
remote processes involved in PDES. This thesis focuses on sequential
simulations as well as distributed memory PDES based on Time Warp
synchronization. Accordingly, this chapter focuses on closely related
work in sequential and distributed memory PDES.

Dickman~\cite{dickman-13} compare event list data structures that
consisted of Splay Tree, STL Multiset and Ladder Queue. However, the
focus of their paper was in developing a framework for handling
pending event set data structure in shared memory PDES. A central
component of their study was the identification of an appropriate data
structure and design for the shared pending event
set. Gupta~\cite{gupta-14} extended their implementation of Ladder
Queue for shared memory Time Warp based simulation environment, so
that it supports lock-free access to events in the shared pending
event set. The modification involved the use of an unsorted lock-free
queue in the underlying Ladder Queue
structure. Marotta~\cite{marotta-16} contributed to the study of
pending event set data structures in threaded PDES through the design
of the Non-Blocking Priority Queue (NBPQ) data structure. A pending
event set data structure that is closely related to Calendar Queues
with constant time performance ~\cite{higiro2017multi}.

Recently, Franceschini~\cite{franceschini-15} compared several
priority-queue based pending event data structures to evaluate their
performance in the context of sequential DEVS simulations. They found
that Ladder Queue outperformed every other priority queue based
pending event data structure such as Sorted List, Minimal List, Binary
Heap, Splay Tree, and Calendar Queue. Tang~\cite{tang-05} and
Franceschini~\cite{franceschini-15} both use the classic Hold
benchmark simulation model used in this research~\cite{higiro2017multi}.

\section{Ladder Queue (ladderQ)}\label{sec:ladderQ}

The Ladder Queue (ladderQ) is a priority queue implementation proposed
by Tang et al~\cite{tang-05} with amortized constant time complexity.
Several investigators have independently verified that for sequential
DES the \textbf{ladderQ} outperforms other priority queues, including:
simple sorted list, binary heap, Splay tree, Calendar queue, and other
multi-list data
structures~\cite{dickman-13,franceschini-15,tang-05}. There are two
key ideas underlying the Ladder Queue, namely: minimize the number of
events to be sorted and delay sorting of events as much as possible.
However, in contrast to the \textbf{ladderQ}, the other data
structures always fix-up and maintain a minimum heap property~\cite{higiro2017multi}.

As shown in Figure~\ref{fig:ladderQ}, the ladder queue consists of
the following 3 substructures:

\begin{enumerate}[leftmargin=*,topsep=0pt]

\item \emph{Top}: An unsorted list which contains events scheduled into the distant future or epoch~\cite{higiro2017multi}.

\item \emph{Ladder}: Consists of multiple rungs, \textit{i.e.,} list
  of buckets. Each bucket contains list of events with a finite range
  of time stamp values. Hence, although events within a bucket are not
  sorted, the buckets on a rung are organized in a sorted order. The
  \textbf{ladderQ} minimizes the number of events to be finally sorted
  by recursively breaking large buckets into smaller buckets in lower
  rungs of its ladder. Lower rungs in the ladder have smaller buckets
  with smaller time ranges and the maximum number of rungs in Ladder
  is 8~\cite{higiro2017multi}.

\item \emph{Bottom}: This substructure contains a sorted list of
  events to be processed. Inserts into \emph{Bottom} must preserve
  sorted order. Hence, the \textbf{ladderQ} strives to maintain a
  short bottom by moving events back into the ladder, as needed. The
  default threshold value at which events from Bottom are moved into
  Ladder is 50~\cite{tang-05,higiro2017multi}.

\end{enumerate}

At the beginning of a simulation, enqueue operations only involve the
insertion of events into \emph{Top}. As the simulation progresses, the
insertion of events can occur at any level of the data structure. The
insertion of events in \emph{Top} and Ladder is an O(1) operation that
involves appending events to a list that remains unsorted. The onset
of dequeue operations involves moving unsorted events from \emph{Top}
into a newly formed rung in Ladder. The time range or bucket-width of
a rung is established by taking the difference between the highest and
lowest time stamp and dividing the difference by the total number of
events. As shown in Figure~\ref{fig:ladderQ}, the bucket-width
computed from the time stamp in \emph{Top} is (6.0\textbf{max} -
1.0\textbf{min})/ 10 = 0.5. In accordance with their
timestamps, events from \emph{Top} are placed into the appropriate
buckets in Rung\textsubscript{1}. In cases, where the number of
events in a bucket exceeds the established threshold, a new rung is
generated to store those events. For example, in
Figure~\ref{fig:ladderQ}, Rung\textsubscript{2} is generated for time stamped
events in the range of 1.0 to 1.5. Next, the bucket
containing events are sequentially removed from the bottom most rung (Rung\textsubscript{2})
in Ladder into the lower substructure. The events are inserted in sorted LTSF order into
\emph{Bottom}, where events are dequeued for further processing. The
clearing of events in Ladder and Bottom kickoffs the movement of
additional events from \emph{Top} into the two lower
substructures. The implementation of Ladder Queue in MUSE adheres to
the functionality described in ~\cite{tang-05} with some
modifications.


\begin{figure} \centering
\begin{minipage}{0.75\linewidth}
\includegraphics[width=\linewidth, height = 12cm]{images/LadderQueue}
\end{minipage}
\textbf{\caption{Structure of Ladder Queue} Source: Tang et al.~\cite{tang-05}}\label{fig:ladderQ}
\end{figure}


\section{Distinguishing aspects of this research}

Our research focuses on distributed memory platforms in which each
parallel process is single threaded. Consequently, our implementation
does not involve thread synchronization issues. However, our 2-tier
design has the ability to further reduce lock contention issues in
multithreaded environments and could provide further performance
boost. To the best of our knowledge, the Fibonnacci heap
(\textbf{fibHeap}) and our 3-tier Heap (\textbf{3tHeap}) are unique
data structures that have potential to be effective in simulations
with high concurrency.

Since it has been established that the Ladder Queue (ladderQ)
outperforms other data structures, we aim to use it for empirical
assessment of our proposed data structures.  However, in contrast to
existing work, rather than using a linked list based implementation, we
propose an alternative implementation using dynamically growing
arrays, that is, std::vector from the C(++) library. Furthermore, we
trigger \textit{Bottom} to \textit{Ladder} re-bucketing only if the
\textit{Bottom} has events at different timestamps to reduce
inefficiencies. Our 2-tier Ladder Queue (\textbf{2tLadderQ}) is a
novel enhancement to the Ladder Queue to enable its efficient use in
optimistic parallel simulations.

\section{Miami University Simulation Environment (MUSE)}
The implementation and assessment of the different data structures was conducted using our parallel simulation framework called Miami University Simulation Environment (MUSE). The application was developed as part of a master's thesis written by Meseret Gebre in the Department of Computer Science at Miami University in 2009~\cite{gebre-2009}. MUSE was developed in C++ and uses the Message Passing Interface (MPI) library for parallel processing. It also uses Time Warp and standard state saving approach to accomplish optimistic synchronization of the LPs to maintain causality in event processing.

\begin{figure}[!tbp]
\centering
\begin{minipage}[b]{0.75\textwidth}
\includegraphics[width=\textwidth]{MUSEarch.jpg}
\textbf{\caption{Overview of a parallel MUSE simulation~\cite{gebre-2009}}}
\label{fig:seesoft}
\end{minipage}

\end{figure}

A conceptual overview of a MUSE-based parallel simulation is shown in Figure~\ref{fig:seesoft}. The simulation kernel implements core functionality associated with LP registration, event processing, state saving, synchronization and Global Virtual Time (GVT) garbage based collection~\cite{higiro2017multi}. Each LP in a simulation maintains an input, output and state queue. The input queue is used to retain events that have already been processed but have not yet been garbage collected. The output queue stores anti-messages, which are events that are sent to other LPs to cancel out previously sent events. The state queue stores the state of the LP at each discrete point in virtual simulation time. A Time Warp LP also maintains a local virtual time (LVT) that is updated to the time-stamp of the event most recently processed by the LP. 

In a Time Warp based simulation such as MUSE, the simulation is organized as a set of LPs that interact with each other by exchanging virtual times-tamped events. LPs process events in non-decreasing receive-time order and generate new events that are transmitted to LPs on local or remote processors. Synchronization of event processing is achieved through the adherence to the local causality constraint, which requires that LPs only process events in Least Time-stamp First (LTSF) order~\cite{jafer-13}. This is in contrast to the conservative synchronization protocol that blocks event processing until it is guaranteed that an LP cannot receive a future event with a receive-time lesser than it's LVT (altogether avoiding the manifestation of causality errors)~\cite{jafer-13}.

A singular advantage of the Time Warp approach in parallel simulations is the ability to withstand violations of the causality constraint. Time Warp LPs proceed optimistically with event processing and during occasions that an LP encounters an event (\textit{named a \textbf{straggler}}) with a receive time lesser than the LVT, a rollback operation is performed. A rollback requires that an LP undo all event processing that occurred at the LVT equal to the straggler time stamp and forward. The LP performs a rollback to a state with an LVT preceding the straggler time stamp and it sends an anti-message to all other agents with the purpose of cancelling the previously sent events.

As shown in Figure~\ref{fig:seesoft}, the kernel also maintains a centralized LTSF scheduler queue for managing pending events and scheduling event processing for local LPs. LPs are permitted to generate events only into the future --\textit{i.e.,} the time stamp on events must be greater than their Local Virtual Time (LVT). Consequently, with a centralized LTSF scheduler, event exchanges between local LPs cannot cause rollbacks. Only events received via MPI can cause rollbacks in our simulation. The scheduler is designed to permit different data structures to be used for managing pending events. This feature is used to experiment with the different pending event scheduler queues discussed in the subsequent chapter. A scheduler queue is required to implement the following key operations to manage pending events~\cite{higiro2017multi}:

\begin{itemize}
\item[\ding{182}] \textbf{Enqueue one or more future events}: This operation adds the given set of events to the pending event set. Multiple events are added to reprocess events after a rollback~\cite{higiro2017multi}.	 

\item[\ding{183}] \textbf{Peek next event}: This operation is expected to return the next event to be processed.  This information is used to determine next LP and to update its LVT prior to event processing. Note that peek does not dequeue events~\cite{higiro2017multi}.

\item[\ding{184}] \textbf{Dequeue events for next LP}: In contrast to peek, this operation is expected to dequeue the events to be dispatched for processing by an LP. This operation is performed by the kernel immediately after a peek operation. The operation must dequeue the next set of concurrent events, \textit{i.e.,} events with the same receive time sent to an LP. However, the concurrent events could have been sent by different LPs on different MPI-processes.
Dispatching concurrent events in a single batch streamlines modeling broad range of scenarios. An total order within concurrent events is not imposed but can be readily introduced if needed~\cite{higiro2017multi}.	

\item[\ding{185}] \textbf{Cancel pending events}: This operation is used as part of rollback recovery process to aggressively remove \emph{all pending events} sent by a given LP (LP\textsubscript{sender}) to another LP (LP\textsubscript{dest}) at-or-after a given time ($t_{rollback}$). In our implementation, only one anti-message with send time $t_{rollback}$ is dispatched to LP\textsubscript{dest} from LP\textsubscript{sender} to cancel prior events sent by LP\textsubscript{sender} to LP\textsubscript{dest} at-or-after $t_{rollback}$. This is a contrast to conventional aggressive cancellation in which one anti-message is generated per event. This feature short circuits the need to send a large number of anti-messages thereby enabling faster rollback recovery. This feature also reduces scans required to cancel events in Ladder Queue data structures. Note that this feature is reliant on the First-In-First-Out (FIFO) communication guarantee provided by MPI~\cite{higiro2017multi}.
\end{itemize}   


\section{Experimental Platform}~\label{sec:platform}
The design of MUSE and the experiments reported were conducted using a distributed-memory compute cluster consisting of 80 compute nodes interconnected by 1 GBPS Ethernet. Each compute node has 8 cores from two quad-core Intel Xeon \textregistered CPUs (E5520) running at 2.27 GHz with hyper-threading disabled. Each compute node has 32 GB of RAM (4 GB per core) in Non-Uniform Memory Access (NUMA) configuration. The cluster has an independent 1 GBPS Ethernet network to support a shared file system. The nodes run Red Hat Enterprise Linux 6, with Linux (kernel ver 2.6.32) and the cluster runs PBS/Torque. The simulation software was compiled using GCC version 4.9.2 with OpenMPI 1.6.4. All debug assertions were turned off for maximum performance~\cite{higiro2017multi}.
