\section{Miami University Simulation Environment (MUSE)} \label{sec:muse}
The implementation and assessment of the different data structures was conducted using our parallel simulation framework called MUSE. It has been developed in C++ and uses the Message Passing Interface (MPI) library for parallel processing. MUSE uses Time Warp and standard state saving approach to accomplish optimistic synchronization of the LPs to maintain causality in event processing.

In a Time Warp based simulation such as MUSE, the simulation is organized as a set of LPs that interact with each other by exchanging virtual times-tamped events. LPs process events in non-decreasing receive-time order and generate new events that are transmitted to LPs on local or remote processors. Synchronization of event processing is achieved through the adherence to the local causality constraint, which requires that LPs only process events in Least Time-stamp First (LTSF) order~\cite{jafer-13}. 

In contrast to the conservative synchronization protocol that blocks event processing until it is guaranteed that an LP cannot receive a future event with a receive-time lesser than it's LVT (altogether avoiding the manifestation of causality errors)~\cite{jafer-13}. Time Warp simulations are able to withstand violations of the causality constraint. Time Warp LPs proceed optimistically with event processing and in the event that it encounters an event (\textit{named a \textbf{straggler}}) with a receive time lesser than the LVT, a rollback operation is performed. A rollback requires that an LP undo all event processing that occurred at the LVT equal to the straggler time stamp and forward. The LP performs a rollback to a state with an LVT preceding the straggler time stamp and it sends an anti-message to all other agents with the purpose of cancelling the previously sent events.  

\begin{figure}[!tbp]
\centering
\begin{minipage}[b]{0.75\textwidth}
\includegraphics[width=\textwidth]{MUSEarch.jpg}
\textbf{\caption{Overview of a parallel MUSE simulation}}
\label{fig:seesoft}
\end{minipage}
\end{figure}

A conceptual overview of a parallel simulation is shown in Figure~\ref{fig:seesoft}. The simulation kernel implements core functionality associated with LP registration, event processing, state saving, synchronization and Global Virtual Time (GVT) garbage based collection. Each LP in a simulation maintains an input, output and state queue. The input queue is used to handle pending events that have yet to be processed. The output queue stores anti-messages, which are events that are sent to other LPs to cancel out previously sent events. The state queue stores the state of the LP at each discrete point in virtual simulation time. A Time Warp LP also maintains a local virtual time (LVT) that is updated to the time-stamp of the event most recently processed by the LP. 

The kernel uses a centralized LTSF scheduler queue for managing pending events and scheduling event processing for local LPs. LPs are permitted to generate events only into the future --\textit{i.e.,} the time stamp on events must be greater than their Local Virtual Time (LVT). Consequently, with a centralized LTSF scheduler, event exchanges between local LPs cannot cause rollbacks. Only events received via MPI can cause rollbacks in our simulation. The scheduler is designed to permit different data structures to be used for managing pending events. This feature is used to experiment with the different pending event scheduler queues discussed in Section~\ref{sec:datastructures}. A scheduler queue is required to implement the following key operations to manage pending events:
	
\begin{itemize}
\item[\ding{182}] \textbf{Enqueue one or more future events}: This operation adds the given set of events to the pending event set. Multiple events are added to reprocess events after a rollback.	 

\item[\ding{183}] \textbf{Peek next event}: This operation is expected to return the next event to be processed.  This information is used to determine next LP and to update its LVT prior to event processing. Note that peek does not dequeue events.

\item[\ding{184}] \textbf{Dequeue events for next LP}: In contrast to peek, this operation is expected to dequeue the events to be dispatched for processing by an LP. This operation is performed by the kernel immediately after a peek operation. The operation must dequeue the next set of concurrent events, \textit{i.e.,} events with the same receive time sent to an LP. However, the concurrent events could have been sent by different LPs on different MPI-processes.
Dispatching concurrent events in a single batch streamlines modeling broad range of scenarios. An total order within concurrent events is not imposed but can be readily introduced if needed.	

\item[\ding{185}] \textbf{Cancel pending events}: This operation is used as part of rollback recovery process to aggressively remove \emph{all pending events} sent by a given LP (LP\textsubscript{sender}) to another LP (LP\textsubscript{dest}) at-or-after a given time ($t_{rollback}$). In our implementation, only one anti-message with send time $t_{rollback}$ is dispatched to LP\textsubscript{dest} from LP\textsubscript{sender} to cancel prior events sent by LP\textsubscript{sender} to LP\textsubscript{dest} at-or-after $t_{rollback}$. This is a contrast to conventional aggressive cancellation in which one anti-message is generated per event. This feature short circuits the need to send a large number of anti-messages thereby enabling faster rollback recovery. This feature also reduces scans required to cancel events in Ladder Queue data structures discussed in Section~\ref{sec:ladderQ} and Section~\ref{sec:2tLadderQ}. Note that this feature is reliant on the First-In-First-Out (FIFO) communication guarantee provided by MPI.
\end{itemize}   

\subsection{Experimental Platform}\label{sec:redhawk}
The design of MUSE and the experiments reported were conducted using a distributed-memory compute cluster consisting of 80 compute nodes interconnected by 1 GBPS Ethernet. Each compute node has 8 cores from two quad-core Intel Xeon \textregistered CPUs (E5520) running at 2.27 GHz with hyper-threading disabled. Each compute node has 32 GB of RAM (4 GB per core) in Non-Uniform Memory Access (NUMA) configuration. The cluster has an independent 1 GBPS Ethernet network to support a shared file system. The nodes run Red Hat Enterprise Linux 6, with Linux (kernel ver 2.6.32) and the cluster runs PBS/Torque. The simulation software was compiled using GCC version 4.9.2 (\textbf{-O3} optimization) with OpenMPI 1.6.4. All debug assertions were turned off for maximum performance.

